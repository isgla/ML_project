# -*- coding: utf-8 -*-
"""Final_Project_CIS519.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GCUxwVY4IIFisujJlsOtuOStfFIplFip

## Load Libraries
"""

!pip uninstall scikit-learn -y
!pip install -U scikit-learn

# Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from scipy.stats import chi2_contingency
from scipy.stats import ttest_ind
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, confusion_matrix
from sklearn import linear_model
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegressionCV 
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import cross_val_score

# Functions

def roc_plot(y, preds):
  roc_auc = roc_auc_score(y, preds[:,1])
  fpr, tpr, thresholds = roc_curve(y, preds[:,1])
  plt.figure()
  plt.plot(fpr, tpr, label='AUC = %0.2f' % roc_auc)
  plt.plot([0, 1], [0, 1],'r--')
  plt.xlim([0.0, 1.0])
  plt.ylim([0.0, 1.05])
  plt.xlabel('False Positive Rate')
  plt.ylabel('True Positive Rate')
  plt.title('Receiver operating characteristic')
  plt.legend(loc="lower right")
  plt.savefig('ROC')
  plt.show()

"""## Import Data"""

from google.colab import drive
drive.mount('/content/drive')

raw_data = pd.read_csv("diabetic_data.csv")
raw_data.head()

# Dictionaries
admission_type_dict = pd.read_csv("admission_type_dict.csv").rename(columns={'description':'admission_type'})
discharge_disposition_dict = pd.read_csv("discharge_disposition_dict.csv").rename(columns={'description':'discharge_disposition'})
admission_source_dict = pd.read_csv("admission_source_dict.csv").rename(columns={'description':'admission_source'})

# Merge dictionaries with raw data
data = raw_data.merge(admission_type_dict, how='left', left_on='admission_type_id', right_on='admission_type_id')\
.merge(discharge_disposition_dict, how='left', left_on='discharge_disposition_id', right_on='discharge_disposition_id')\
.merge(admission_source_dict, how='left', left_on='admission_source_id', right_on='admission_source_id')

data.drop(columns=['admission_type_id','discharge_disposition_id','admission_source_id'], inplace=True)

# Merge CCS Descriptions for Diagnoses
ccs_icd9_diagnoses = pd.read_excel("CCS_DIAGNOSES.xlsx", header=0, converters={'ICD9_CODE':str,'DX_NAME':str,'CCS':str,'CCS_DESC':str})

data = data.merge(ccs_icd9_diagnoses, how='left', left_on='diag_1', right_on='ICD9_CODE')
data.rename(columns={'DX_NAME':'diag_1_desc','CCS':'ccs_1_code','CCS_DESC':'ccs_1_desc'}, inplace=True)
data.drop(columns='ICD9_CODE',inplace=True)

data = data.merge(ccs_icd9_diagnoses, how='left', left_on='diag_2', right_on='ICD9_CODE')
data.rename(columns={'DX_NAME':'diag_2_desc','CCS':'ccs_2_code','CCS_DESC':'ccs_2_desc'}, inplace=True)
data.drop(columns='ICD9_CODE',inplace=True)

data = data.merge(ccs_icd9_diagnoses, how='left', left_on='diag_3', right_on='ICD9_CODE')
data.rename(columns={'DX_NAME':'diag_3_desc','CCS':'ccs_3_code','CCS_DESC':'ccs_3_desc'}, inplace=True)
data.drop(columns='ICD9_CODE',inplace=True)

"""## Cleaning/Preprocessing"""

# Fill Missing CCS Diagnosis Categories with "Other" 
data['ccs_1_desc'].fillna('Other',inplace=True)
data['ccs_2_desc'].fillna('Other',inplace=True)
data['ccs_3_desc'].fillna('Other',inplace=True)

# Drop Remaining Diagnosis Columns
data.drop(columns=['diag_1','diag_1_desc','ccs_1_code','diag_2','diag_2_desc','ccs_2_code','diag_3','diag_3_desc','ccs_3_code'], inplace=True)

# Create readmission target variable
data['readmit_yn'] = np.where(data.readmitted=='<30',1,0)

# Exclude patients with a discharge disposition of expired since they cannot be readmitted
data = data[~data['discharge_disposition'].isin(['Expired','Expired at home. Medicaid only, hospice.','Expired in a medical facility. Medicaid only, hospice.'])]

# Exclude patients missing outcome 
data.dropna(subset=['readmitted'], inplace=True)

# Randomly Select One Record Per Patient
data = data.sample(frac=1, random_state=42).reset_index(drop=True)
data.drop_duplicates('patient_nbr', keep='first', inplace=True)

data.shape

data.head()

# Types of data
data.info()

# Missing Data
pd.DataFrame(data.replace('None',np.NAN).replace('?',np.NAN).isna().sum()/len(data)).sort_values(0, ascending=False)

# Create Indicator Fields for Presence Weight
# data['weight'].unique() 
data['weight'] = data['weight'].replace('?',np.nan)
data['weight_yn'] = np.where(data.weight.isna(),0,1)
data[['weight','weight_yn']]

data['max_glu_serum'].unique()
data['payer_code'].unique()

# Create Missing Category for Payor, Race, Max Serum Glucose, A1c, Medical Specialty
data['payer_code'] = data['payer_code'].replace('?',np.nan).fillna('Unknown')
data['race'] = data['race'].replace('?',np.nan).fillna('Unknown')
data['A1Cresult'] = data['A1Cresult'].fillna('None')
data['max_glu_serum'] = data['max_glu_serum'].fillna('None')
data['medical_specialty'] = data['medical_specialty'].fillna('None')

# Collapse Categories for Specialty, Discharge Disposition
data['medical_specialty_category'] = np.where(data.medical_specialty.isin(['InternalMedicine','Family/GeneralPractice']),'Internal/Family Medicine',
                                              np.where(data.medical_specialty.isin(['Orthopedics-Reconstructive','Orthopedics']),'Orthopedics',
                                                       np.where(data.medical_specialty.isin(['Obstetrics','ObstetricsandGynecology','Gynecology']),'OBGYN',
                                                                np.where(data.medical_specialty.isin(['Hematology','Oncology','Hematology/Oncology','Pediatrics-Hematology-Oncology']),'Hematology/Oncology',
                                                                         np.where(data.medical_specialty.isin(['Psychiatry-Addictive','Psychiatry-Child/Adolescent','Psychiatry']),'Psychiatry',
                                                                                  np.where(data.medical_specialty.isin(['Cardiology','Cardiology-Pediatric']),'Cardiology',
                                                                                           np.where(data.medical_specialty.isin(['Pulmonology','Pediatrics-Pulmonology']),'Pulmonology',
                                                                                                    np.where(data.medical_specialty.isin(['Emergency/Trauma']),'Emergency/Trauma',
                                                                                                             np.where(data.medical_specialty.isin(['Surgery-General','Surgery-Neuro',
                                                                                                                                                  'Surgery-Cardiovascular/Thoracic',
                                                                                                                                                   'Surgery-Vascular','Surgery-Cardiovascular',
                                                                                                                                                   'Surgery-Thoracic','Surgery-Plastic',
                                                                                                                                                   'Surgery-Pediatric','Surgery-Maxillofacial',
                                                                                                                                                   'Surgery-Colon&Rectal','Surgeon',
                                                                                                                                                   'Surgery-PlasticwithinHeadandNeck']),'Surgery',
                                                                                                                      'Unknown/Other')))))))))

data['discharge_disposition_category'] = np.where(data.discharge_disposition.isna(),'Unknown/Other',
                                                  np.where(data.discharge_disposition=='Discharged to home','Discharged to home',
                                                           np.where(data.discharge_disposition.isin(['Discharged/transferred to home with home health service',
                                                                                                     'Discharged/transferred to home under care of Home IV provider',
                                                                                                     ]),'Discharged to home health',
                                                                    np.where(data.discharge_disposition.isin(['Hospice / medical facility',
                                                                                                     'Hospice / home'
                                                                                                     ]),'Discharged to hospice',  
                                                                             np.where(data.discharge_disposition.isin(['Discharged/transferred to SNF',
                                                                                                                       'Discharged/transferred to another short term hospital',
                                                                                                                       'Discharged/transferred to another rehab fac including rehab units of a hospital',
                                                                                                                       'Discharged/transferred to another type of inpatient care institution',
                                                                                                                       'Discharged/transferred to ICF',
                                                                                                                       'Discharged/transferred to a long term care hospital.',
                                                                                                                       'Hospice / medical facility',
                                                                                                                       'Discharged/transferred/referred to a psychiatric hospital of psychiatric distinct part unit of a hospital',
                                                                                                                       'Discharged/transferred within this institution to Medicare approved swing bed',
                                                                                                                       'Discharged/transferred to a nursing facility certified under Medicaid but not certified under Medicare.',
                                                                                                                       'Admitted as an inpatient to this hospital',
                                                                                                                       'Neonate discharged to another hospital for neonatal aftercare',
                                                                                                                       'Discharged/transferred to a federal health care facility.'
                                                                                                     ]),'Discharged to facility','Unknown/Other')))))                                                                                                                                       


data.drop(columns=['medical_specialty','discharge_disposition'], inplace=True)

# Create NA Categories for Admission Type
data['admission_type'] = np.where(data.admission_type.isin(['Not Available','Not Mapped']) | data.admission_type.isna(),'NA',data.admission_type)

"""# Exploratory Data Analysis"""

# Checking frequency of missing data
pd.DataFrame(data.replace('None',np.NAN).replace('?',np.NAN).isna().sum()/len(data)).sort_values(0, ascending=False)

warnings.filterwarnings("ignore")

# Exploratory Analysis
fig, ax = plt.subplots()
ax.hist(data['time_in_hospital'], bins=14, edgecolor="black")
ax.set_title('Length of Stay (Days)')
plt.show()

# Density Plots
for gender in data['gender'].unique():
  if gender == 'Unknown/Invalid':
    continue
  subset = data[data['gender']==gender]
  
  sns.distplot(subset['time_in_hospital'], hist = False, kde = True,
                 kde_kws = {'shade': True, 'linewidth': 3}, 
                  label = gender)

plt.legend(prop={'size': 12}, title = 'Gender', loc=6, borderaxespad=30.)
plt.show()

for race in data['race'].unique():
  if race == '?':
    continue
  subset = data[data['race']==race]
  
  sns.distplot(subset['time_in_hospital'], hist = False, kde = True,
                 kde_kws = {'shade': True, 'linewidth': 3}, 
                  label = race)

plt.legend(prop={'size': 12}, title = 'Gender', loc=6, borderaxespad=30.)
plt.show()

for age in ['[0-10)','[10-20)', '[20-30)','[30-40)','[40-50)','[50-60)','[60-70)','[70-80)','[80-90)','[90-100)']:
  subset = data[data['age']==age]
  
  sns.distplot(subset['time_in_hospital'], hist = False, kde = True,
                 kde_kws = {'shade': True, 'linewidth': 3}, 
                  label = age)

plt.legend(prop={'size': 12}, title = 'Gender', loc=6, borderaxespad=30.)
plt.show()



pal = ['steelblue','grey','red']
sns.countplot(x='readmitted', data=data, palette=pal, alpha=0.7)
plt.title('Readmissions')
plt.ylabel('Freq')
plt.xlabel('Status')
plt.show()

# Correlation matrix
correlation_mat = data.corr()
sns.heatmap(correlation_mat)
plt.show()
# Correlation matrix only showing the ones with type int
# should we change to #'s ?

# Continuous Variables

# box plots of distributions 

pal = ['sandybrown','grey','steelblue']

fig, ax  = plt.subplots(1,3, figsize=(16,6))
sns.boxplot(x=data['readmitted'], y=data['number_emergency'], hue=data['readmitted'], data=data, ax=ax[0], palette=pal)
sns.boxplot(x=data['readmitted'], y=data['number_inpatient'], hue=data['readmitted'], data=data, ax=ax[1], palette=pal)
sns.boxplot(x=data['readmitted'], y=data['number_outpatient'], hue=data['readmitted'], data=data, ax=ax[2], palette=pal)
# ax[0].legend(loc=9)
ax[0].set_title('ED Visits (Previous Year)')
ax[0].set_xlabel('Readmissions Status')
ax[0].set_ylabel('Number')
ax[1].set_title('Inpatient Admissions (Previous Year)')
ax[1].set_xlabel('Readmissions Status')
ax[1].set_ylabel('Number')
ax[2].set_title('Outpatient Visits (Previous Year)')
ax[2].set_xlabel('Readmissions Status')
ax[2].set_ylabel('Number')
plt.show()

fig, ax  = plt.subplots(1,4, figsize=(16,6))
sns.boxplot(x=data['readmitted'], y=data['num_lab_procedures'], hue=data['readmitted'], data=data, ax=ax[0], palette=pal)
sns.boxplot(x=data['readmitted'], y=data['num_procedures'], hue=data['readmitted'], data=data, ax=ax[1], palette=pal)
sns.boxplot(x=data['readmitted'], y=data['num_medications'], hue=data['readmitted'], data=data, ax=ax[2], palette=pal)
sns.boxplot(x=data['readmitted'], y=data['number_diagnoses'], hue=data['readmitted'], data=data, ax=ax[3], palette=pal)
# ax[0].legend(loc=9)
ax[0].set_title('Lab Procedures')
ax[0].set_xlabel('Readmissions Status')
ax[0].set_ylabel('Number')
ax[1].set_title('Procedures')
ax[1].set_xlabel('Readmissions Status')
ax[1].set_ylabel('Number')
ax[2].set_title('Medications Administered')
ax[2].set_xlabel('Readmissions Status')
ax[2].set_ylabel('Number')
ax[3].set_title('Diagnoses')
ax[3].set_xlabel('Readmissions Status')
ax[3].set_ylabel('Number')
plt.show()

# Categorical Variables

pal = ['sandybrown','grey','steelblue']
fig, ax  = plt.subplots(2,2, figsize=(16,16))
sns.countplot(data=data, y="gender", hue="readmitted", ax=ax[0,0], palette=pal)
ax[0,0].legend(loc=4)
sns.countplot(data=data, y="race", hue="readmitted", ax=ax[0,1], palette=pal)
ax[0,1].legend(loc=4)
sns.countplot(data=data, y="age", hue="readmitted", ax=ax[1,0], palette=pal, order=['[0-10)','[10-20)', '[20-30)',
       '[30-40)','[40-50)','[50-60)','[60-70)','[70-80)','[80-90)','[90-100)'])
ax[1,0].legend(loc=4)
sns.countplot(data=data, y="payer_code", hue="readmitted", ax=ax[1,1], palette=pal)
ax[1,1].legend(loc=4)
plt.show()

# Chi-square tests for significant associations

cols = data.columns[data.dtypes=='object']

for col in cols:
  if col=='readmitted':
    continue
  crosstab = pd.crosstab(data[col], data['readmit_yn'])
  pval = chi2_contingency(crosstab)[1]
  if pval < 0.05/len(cols):
    print(crosstab.apply(lambda r: r/r.sum(), axis=1))
    print('p-val: ',"{0:.6f}".format(pval),'\n')

# T-tests for continuous variables
cols = data.select_dtypes(include=['int']).columns

with np.errstate(divide='ignore'):
  res = []
  for col in cols:
    if col=='readmit_yn':
      continue
    mean_readmitted = data[data['readmit_yn']==1][col].mean()
    mean_not_readmitted = data[data['readmit_yn']==0][col].mean()
    try:
      tt_res = ttest_ind(data[data['readmit_yn']==1][col],
                        data[data['readmit_yn']==0][col],
                        nan_policy='omit')
      pval = tt_res[1]
      if pval < 0.05/len(cols):
        new_res = {'feature':col,
                  'Mean, Readmitted':mean_readmitted, 
                  'Mean, Not Readmitted': mean_not_readmitted,
                  'p-val':"{0:.8f}".format(pval)}
        res.append(new_res)

    except:
      continue

# Display features
pd.DataFrame(res).sort_values('p-val')

"""# Splitting Data into training (60%), validation (10%), and testing (30%)


"""

final = data.copy()
final = final.sample(frac=1, random_state=123).reset_index(drop=True)

# Split Train, Test, and Validation Sets
test_size = int(final.shape[0] * 0.3)
train_size = int(final.shape[0] * 0.6)

train_ids = final.head(train_size).index.unique()
test_ids = final.tail(test_size).index.unique()
valid_ids = final[(~final.index.isin(test_ids)) & (~final.index.isin(train_ids))].index.unique()

train = final.loc[train_ids]
valid = final.loc[valid_ids]
test = final.loc[test_ids]

print('Num train in test:',sum(train.index.isin(test_ids)))
print('Num test in train:',sum(test.index.isin(train_ids)))
print('Num valid in train:',sum(valid.index.isin(train_ids)))
print('Num valid in test:',sum(valid.index.isin(test_ids)))

train.shape, valid.shape, test.shape

[(train.columns.get_loc(col),col) for col in train.columns]

"""**Analysis**

Features we will use: \\
       'race', 'gender', 'age', 'weight',
       'time_in_hospital', 'payer_code', 'num_lab_procedures',
       'num_procedures', 'num_medications', 'number_outpatient',
       'number_emergency', 'number_inpatient', 'number_diagnoses',
       'max_glu_serum', 'A1Cresult', 'metformin', 'repaglinide', 'nateglinide',
       'chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide',
       'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose',
       'miglitol', 'troglitazone', 'tolazamide', 'examide', 'citoglipton',
       'insulin', 'glyburide-metformin', 'glipizide-metformin',
       'glimepiride-pioglitazone', 'metformin-rosiglitazone',
       'metformin-pioglitazone', 'change', 'diabetesMed',
       'admission_type', 'discharge_disposition_category', 'ccs_1_desc', 
       'weight_yn', 'medical_specialty_category'
"""

# Train: Separate features and label
X_train = train[['race','gender','age','weight_yn','weight','ccs_1_desc','number_outpatient','number_emergency','number_inpatient','number_diagnoses',
                 'medical_specialty_category','max_glu_serum', 'A1Cresult', 'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 
                 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 
                 'tolazamide', 'examide', 'citoglipton', 'insulin', 'glyburide-metformin', 'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', 
                 'metformin-pioglitazone', 'change', 'diabetesMed', 'admission_type', 'discharge_disposition_category']]
y_train = train['readmit_yn']
# y_train = train['time_in_hospital']

# Validation: Separate features and label
X_valid = valid[['race','gender','age','weight_yn','weight','ccs_1_desc','number_outpatient','number_emergency','number_inpatient','number_diagnoses',
                 'medical_specialty_category','max_glu_serum', 'A1Cresult', 'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 
                 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 
                 'tolazamide', 'examide', 'citoglipton', 'insulin', 'glyburide-metformin', 'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', 
                 'metformin-pioglitazone', 'change', 'diabetesMed', 'admission_type', 'discharge_disposition_category']]
y_valid = valid['readmit_yn']
# y_valid = valid['time_in_hospital']

# Test: Separate features and label
X_test = test[['race','gender','age','weight_yn','weight','ccs_1_desc','number_outpatient','number_emergency','number_inpatient','number_diagnoses',
                 'medical_specialty_category','max_glu_serum', 'A1Cresult', 'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 
                 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 
                 'tolazamide', 'examide', 'citoglipton', 'insulin', 'glyburide-metformin', 'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', 
                 'metformin-pioglitazone', 'change', 'diabetesMed', 'admission_type', 'discharge_disposition_category']]
y_test = test['readmit_yn']
# y_test = test['time_in_hospital']

"""## Imputation"""

warnings.filterwarnings("ignore")
# Weight is provided as a categorical variable, so we determine the mode across age groups
mode_weight_0_10 = data[data.age == '[0-10)'].weight.mode()
mode_weight_10_20 = data[data.age == '[10-20)'].weight.mode()
mode_weight_other = data[~data.age.isin(['[0-10)','[10-20)'])].weight.mode()
 
X_train['weight'] = np.where(X_train.weight.isna() & X_train.age == '[0-10)', mode_weight_0_10,
                             np.where(X_train.weight.isna() & X_train.age == '[10-20)', mode_weight_10_20,
                                      np.where(X_train.weight.isna() & ~X_train.age.isin(['[0-10)','[10-20)']), mode_weight_other,X_train.weight)))

X_valid['weight'] = np.where(X_valid.weight.isna() & X_valid.age == '[0-10)', mode_weight_0_10,
                             np.where(X_valid.weight.isna() & X_valid.age == '[10-20)', mode_weight_10_20,
                                      np.where(X_valid.weight.isna() & ~X_valid.age.isin(['[0-10)','[10-20)']), mode_weight_other,X_valid.weight)))

X_test['weight'] = np.where(X_test.weight.isna() & X_test.age == '[0-10)', mode_weight_0_10,
                             np.where(X_test.weight.isna() & X_test.age == '[10-20)', mode_weight_10_20,
                                      np.where(X_test.weight.isna() & ~X_test.age.isin(['[0-10)','[10-20)']), mode_weight_other,X_test.weight)))

"""## One-hot Encoding"""

# One-hot encoding
X_train = pd.get_dummies(X_train, columns=['race','gender','age','ccs_1_desc','weight','medical_specialty_category','max_glu_serum', 'A1Cresult', 'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 
                 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 
                 'tolazamide', 'examide', 'citoglipton', 'insulin', 'glyburide-metformin', 'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', 
                 'metformin-pioglitazone', 'change', 'diabetesMed', 'admission_type', 'discharge_disposition_category'])
X_valid = pd.get_dummies(X_valid, columns=['race','gender','age','ccs_1_desc','weight','medical_specialty_category','max_glu_serum', 'A1Cresult', 'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 
                 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 
                 'tolazamide', 'examide', 'citoglipton', 'insulin', 'glyburide-metformin', 'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', 
                 'metformin-pioglitazone', 'change', 'diabetesMed', 'admission_type', 'discharge_disposition_category'])
X_test = pd.get_dummies(X_test, columns=['race','gender','age','ccs_1_desc','weight','medical_specialty_category','max_glu_serum', 'A1Cresult', 'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 
                 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 
                 'tolazamide', 'examide', 'citoglipton', 'insulin', 'glyburide-metformin', 'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', 
                 'metformin-pioglitazone', 'change', 'diabetesMed', 'admission_type', 'discharge_disposition_category'])

X_train, X_valid = X_train.align(X_valid, join='left', axis=1)
X_train, X_test = X_train.align(X_test, join='left', axis=1)

# print(X_train.shape)
# print(X_valid.shape)
# print(X_test.shape)

# Fill Missing Diagnoses with 0's
X_valid.iloc[:,5:] = X_valid.iloc[:,5:].fillna(0)
X_test.iloc[:,5:] = X_test.iloc[:,5:].fillna(0)

"""## Standardization
We standardized features for regularized regression.
"""

# Scale data
scaler = StandardScaler()
X_train_std = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)

# Scale validation and test sets
X_valid_std = pd.DataFrame(scaler.transform(X_valid), columns=X_valid.columns)
X_test_std = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)

"""# Modeling: Length of Stay"""

# Commented out IPython magic to ensure Python compatibility.
# Modeling 
# Steps followed: Choose an algorithm, overfit, reduce overfit with regularization

# *** For length of stay (days) in the hospital, we will use linear regression ***
# *** Linear model with no regularization ***
regr = linear_model.LinearRegression()
#y_train = y_train.values.ravel()
# Train the model using the training sets
regr.fit(X_train_std, y_train)

# Make predictions using the testing set
y_pred = regr.predict(X_test_std)
print(y_test)
print(y_pred)

from sklearn.metrics import r2_score
print(r2_score(y_train, y_pred_train))
print(r2_score(y_test, y_pred))

# The coefficients
# print('Coefficients: \n', regr.coef_)
# The mean squared error
print('Mean squared error: %.2f'
#       % mean_squared_error(y_test, y_pred))
# The coefficient of determination: 1 is perfect prediction
print('Coefficient of determination: %.2f'
#       % r2_score(y_test, y_pred))

scores_length_no_reg = cross_val_score(regr, X_train_std, y_train, cv=5, scoring='r2') 
regr.fit(X_train_std, y_train)
#The mean score and the standard deviation are hence given by:
print("%0.2f (No regularization) accuracy with a standard deviation of %0.2f" % (scores_length_no_reg.mean(), scores_length_no_reg.std()))

# *** linear model with L1 regularization ***

# Alpha = 0.1
regr_l1_1 = linear_model.Lasso(alpha=0.1)
scores_length_l1_1_reg = cross_val_score(regr_l1_1, X_train_std, y_train, cv=5, scoring='r2') 
regr_l1_1.fit(X_train_std, y_train)
#The mean score and the standard deviation are hence given by:
print("%0.2f (with L1 alpha = 0.1) accuracy with a standard deviation of %0.2f" % (scores_length_l1_1_reg.mean(), scores_length_l1_1_reg.std()))

# Alpha = 30
regr_l1_30 = linear_model.Lasso(alpha=30)
scores_length_l1_30_reg = cross_val_score(regr_l1_30, X_train_std, y_train, cv=5, scoring='r2') 
regr_l1_30.fit(X_train_std, y_train)
#The mean score and the standard deviation are hence given by:
print("%0.2f (with L1 alpha = 30) accuracy with a standard deviation of %0.2f" % (scores_length_l1_30_reg.mean(), scores_length_l1_30_reg.std()))

# Alpha = 100
regr_l1_100 = linear_model.Lasso(alpha=100)
scores_length_l1_100_reg = cross_val_score(regr_l1_100, X_train_std, y_train, cv=5, scoring='r2') 
#regr_l1_30.fit(X_train_std, y_train)
#The mean score and the standard deviation are hence given by:
print("%0.2f (with L1 alpha = 100) accuracy with a standard deviation of %0.2f" % (scores_length_l1_100_reg.mean(), scores_length_l1_100_reg.std()))

# *** linear model with L2 regularization ***

# Alpha = 0.1
regr_l2_1 = linear_model.Ridge(alpha=0.1)
scores_length_l2_1_reg = cross_val_score(regr_l2_1, X_train_std, y_train, cv=5, scoring='r2') 
regr_l2_1.fit(X_train_std, y_train)
#The mean score and the standard deviation are hence given by:
print("%0.2f (with L2 alpha = 0.1) accuracy with a standard deviation of %0.2f" % (scores_length_l2_1_reg.mean(), scores_length_l2_1_reg.std()))

# Alpha = 30
regr_l2_30 = linear_model.Ridge(alpha=30)
scores_length_l2_30_reg = cross_val_score(regr_l2_30, X_train_std, y_train, cv=5, scoring='r2') 
regr_l2_30.fit(X_train_std, y_train)
#The mean score and the standard deviation are hence given by:
print("%0.2f (with L2 alpha = 30) accuracy with a standard deviation of %0.2f" % (scores_length_l2_30_reg.mean(), scores_length_l2_30_reg.std()))

# Alpha = 100
regr_l2_100 = linear_model.Ridge(alpha=100)
scores_length_l2_100_reg = cross_val_score(regr_l2_100, X_train_std, y_train, cv=5, scoring='r2') 
regr_l2_100.fit(X_train_std, y_train)
#print(scores_length_l2_100_reg)
#The mean score and the standard deviation are hence given by:
print("%0.2f (with L2 alpha = 100) accuracy with a standard deviation of %0.2f" % (scores_length_l2_100_reg.mean(), scores_length_l2_100_reg.std()))
#print(patient)

# Commented out IPython magic to ensure Python compatibility.
# Modeling with Poisson Regressor

import sklearn
from sklearn.linear_model import PoissonRegressor
regr = PoissonRegressor(alpha=1.0, fit_intercept=True, max_iter=100, tol=0.0001, warm_start=False, verbose=0)
regr.fit(X_train_std, y_train)

# Make predictions using the testing set
y_pred = regr.predict(X_test_std)

from sklearn.metrics import r2_score
print(r2_score(y_test, y_pred))

# The coefficients
# print('Coefficients: \n', regr.coef_)
# The mean squared error
print('Mean squared error: %.2f'
#       % mean_squared_error(y_test, y_pred))
# The coefficient of determination: 1 is perfect prediction
print('Coefficient of determination: %.2f'
#       % r2_score(y_test, y_pred))

scores_length_no_reg = cross_val_score(regr, X_train_std, y_train, cv=5, scoring='r2') 
regr.fit(X_train_std, y_train)
#The mean score and the standard deviation are hence given by:
print("%0.2f (alpha = 1.0) accuracy with a standard deviation of %0.2f" % (scores_length_no_reg.mean(), scores_length_no_reg.std()))

# Alpha = 0.1
regr_l1_1 = linear_model.PoissonRegressor(alpha=0.01)
scores_length_l1_1_reg = cross_val_score(regr_l1_1, X_train_std, y_train, cv=5, scoring='r2') 
regr_l1_1.fit(X_train_std, y_train)
#The mean score and the standard deviation are hence given by:
print("%0.2f (alpha = 0.01) accuracy with a standard deviation of %0.2f" % (scores_length_l1_1_reg.mean(), scores_length_l1_1_reg.std()))

# Alpha = 30
regr_l1_30 = linear_model.PoissonRegressor(alpha=30)
scores_length_l1_30_reg = cross_val_score(regr_l1_30, X_train_std, y_train, cv=5, scoring='r2') 
regr_l1_30.fit(X_train_std, y_train)
#The mean score and the standard deviation are hence given by:
print("%0.2f (alpha = 30) accuracy with a standard deviation of %0.2f" % (scores_length_l1_30_reg.mean(), scores_length_l1_30_reg.std()))

# Alpha = 100
regr_l1_100 = linear_model.PoissonRegressor(alpha=0.001)
scores_length_l1_100_reg = cross_val_score(regr_l1_100, X_train_std, y_train, cv=5, scoring='r2') 
#regr_l1_30.fit(X_train_std, y_train)
#The mean score and the standard deviation are hence given by:
print("%0.2f (alpha = 100) accuracy with a standard deviation of %0.2f" % (scores_length_l1_100_reg.mean(), scores_length_l1_100_reg.std()))

"""# Modeling: 30-Day Readmissions"""

# For 30-day readmission, we will use logistic regression, gradient boosting, and random forest classifiers.
# Process

# Standardize features
# Compute mean and stdev on training set for standardization

# Logistic regression

# Covalidation score

# Gradient boosting

"""## Logistic Regression with L1 Regularization"""

# Set parameters
n_folds = 5
C_values = [0.001,0.01,1]

# Fit model
lrcv_l1_clf = LogisticRegressionCV(Cs=C_values, cv=n_folds, penalty='l1', 
                           refit=False, scoring='roc_auc', max_iter=50,
                           solver='liblinear', random_state=42,
                           fit_intercept=False, n_jobs=-1)

lrcv_l1_clf.fit(X_train_std, y_train.values.ravel())

print('CV Max AUC:', lrcv_l1_clf.scores_[1].max())

# Coefficients

feature_list = list(X_train_std.columns)
coefficients = lrcv_l1_clf.coef_[0]

feature_coefficients = list(zip(feature_list, coefficients))
coef_list = [list(x) for x in feature_coefficients]
coef_df = pd.DataFrame(coef_list, columns=["Variable", "Coefficient"])
coef_df['Abs Coef'] = abs(coef_df['Coefficient'])
coef_df['OR'] = np.exp(coef_df['Coefficient'])


# Display Features with Odds Ratios
coef_df[coef_df['Abs Coef']>0.1].sort_values('Abs Coef', ascending=False)[['Variable','Coefficient','OR']]

valid_preds = pd.Series(lrcv_clf.predict_proba(X_valid_std)[:,1])
valid_preds = valid_preds.values
valid_preds = np.array([valid_preds,valid_preds]).T
roc_plot(y_valid,valid_preds)

"""## Logistic Regression with L2 Regularization"""

# Set parameters
n_folds = 5
C_values = [0.001,0.01,1]

# Fit model
lrcv_l2_clf = LogisticRegressionCV(Cs=C_values, cv=n_folds, penalty='l2', 
                           refit=False, scoring='roc_auc', max_iter=50,
                           solver='liblinear', random_state=42,
                           fit_intercept=False, n_jobs=-1)

lrcv_l2_clf.fit(X_train_std, y_train.values.ravel())

print('CV Max AUC:', lrcv_l2_clf.scores_[1].max())

# Coefficients

feature_list = list(X_train_std.columns)
coefficients = lrcv_l2_clf.coef_[0]

feature_coefficients = list(zip(feature_list, coefficients))
coef_list = [list(x) for x in feature_coefficients]
coef_df = pd.DataFrame(coef_list, columns=["Variable", "Coefficient"])
coef_df['Abs Coef'] = abs(coef_df['Coefficient'])
coef_df['OR'] = np.exp(coef_df['Coefficient'])


# Display Features with Odds Ratios
coef_df.sort_values('Abs Coef', ascending=False)[['Variable','Coefficient','OR']][0:25]

valid_preds = pd.Series(lrcv_l2_clf.predict_proba(X_valid_std)[:,1])
valid_preds = valid_preds.values
valid_preds = np.array([valid_preds,valid_preds]).T
roc_plot(y_valid,valid_preds)

"""## Random Forest"""

# Random Forest Classifier

rfc=RandomForestClassifier(random_state=123)

param_grid = { 
    'n_estimators': [100, 200, 500],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth' : [4,6,8,10,12],
    'criterion' :['entropy']
}

CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)
CV_rfc.fit(X_train, y_train.values.ravel())

CV_rfc.best_params_

# Display Feature Importances
importances = CV_rfc.best_estimator_.feature_importances_

indices = np.argsort(importances)[::-1]

print("Feature ranking:")
for f in range(X_train.shape[1]):
  print("%d. feature %s (%f)" % (f + 1, X_train.columns[indices[f]], importances[indices[f]]))

valid_preds = pd.Series(CV_rfc.predict_proba(X_valid)[:,1])
valid_preds = valid_preds.values
valid_preds = np.array([valid_preds,valid_preds]).T
roc_plot(y_valid,valid_preds)

"""## Gradient Boosting Classifier"""

# Gradient Boosting Classifier
gbc=GradientBoostingClassifier(random_state=123)

param_grid = { 
    'n_estimators': [200, 500],
    'max_features': ['sqrt', 'log2'],
    'max_depth' : [4,8,12,16],
    'learning_rate': [0.005,0.01,0.1,1]
}

CV_gbc = GridSearchCV(estimator=gbc, param_grid=param_grid, scoring='roc_auc', cv= 5)
CV_gbc.fit(X_train, y_train.values.ravel())

CV_gbc.best_params_

# Display Feature Importances
importances = CV_gbc.best_estimator_.feature_importances_

indices = np.argsort(importances)[::-1]

print("Feature ranking:")
for f in range(X_train.shape[1]):
  print("%d. feature %s (%f)" % (f + 1, X_train.columns[indices[f]], importances[indices[f]]))

valid_preds = pd.Series(CV_gbc.predict_proba(X_valid)[:,1])
valid_preds = valid_preds.values
valid_preds = np.array([valid_preds,valid_preds]).T
roc_plot(y_valid,valid_preds)

# Analysis and Evaluation 

# Do cross validation in every algorithm

# Evaluation metrics 

# Feature importance

# Training/ Inference/ Time/ Cost

# Comparison to other models

# Least confident examples

# Bias/ Variance trade-off

"""NOTES:

It is okay to use existing libraries in your project implementation, as long as a) you are citing all sources and b) you are still showing a proficiency with some of the concepts learned in class (whether they be particular approaches, necessities of data processing, or evaluation techniques)

https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html

https://whimsical.com/machine-learning-roadmap-2020-CA7f3ykvXpnJ9Az32vYXva

"""